{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38cde99e-aaca-425f-9698-5c53cab15b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Dataset ===\n",
      "Train File: train_subtask1.csv -> 2925 samples, 6 columns\n",
      "Dev File  : dev_subtask1.csv -> 323 samples, 6 columns\n",
      "Test File : test_subtask1_text.csv -> 311 samples, 2 columns\n",
      "\n",
      "=== Training Model: TF-IDF + SVM-Linear ===\n",
      "Loading TF-IDF embeddings...\n",
      "Initializing SVM-Linear model...\n",
      "Best Hyperparameters: C=1.0, kernel='linear'\n",
      "10-Fold CV -> Accuracy: 91.33 | Precision: 91.31 | Recall: 91.33 | F1: 91.3\n",
      "\n",
      "=== Training Model: TF-IDF + LogisticRegression ===\n",
      "Loading TF-IDF embeddings...\n",
      "Initializing LogisticRegression model...\n",
      "Best Hyperparameters: solver='lbfgs', max_iter=1000\n",
      "10-Fold CV -> Accuracy: 86.45 | Precision: 86.31 | Recall: 86.45 | F1: 86.23\n",
      "\n",
      "=== Training Model: TF-IDF + NaiveBayes ===\n",
      "Loading TF-IDF embeddings...\n",
      "Initializing NaiveBayes model...\n",
      "Best Hyperparameters: alpha=1.0\n",
      "10-Fold CV -> Accuracy: 77.52 | Precision: 77.18 | Recall: 77.25 | F1: 77.33\n",
      "\n",
      "=== Training Model: BoW + SVM-Linear ===\n",
      "Loading BoW embeddings...\n",
      "Initializing SVM-Linear model...\n",
      "Best Hyperparameters: C=0.5, kernel='linear'\n",
      "10-Fold CV -> Accuracy: 83.32 | Precision: 83.1 | Recall: 83.32 | F1: 83.01\n",
      "\n",
      "=== Training Model: BoW + LogisticRegression ===\n",
      "Loading BoW embeddings...\n",
      "Initializing LogisticRegression model...\n",
      "Best Hyperparameters: solver='lbfgs', max_iter=1000\n",
      "10-Fold CV -> Accuracy: 78.94 | Precision: 85.66 | Recall: 78.94 | F1: 78.46\n",
      "\n",
      "=== Training Model: BoW + NaiveBayes ===\n",
      "Loading BoW embeddings...\n",
      "Initializing NaiveBayes model...\n",
      "Best Hyperparameters: alpha=1.0\n",
      "10-Fold CV -> Accuracy: 72.1 | Precision: 72.3 | Recall: 72.0 | F1: 72.05\n",
      "\n",
      "=== Training Model: Word2Vec + SVM-Linear ===\n",
      "Loading Word2Vec embeddings...\n",
      "Initializing SVM-Linear model...\n",
      "Best Hyperparameters: C=1.0, kernel='linear'\n",
      "10-Fold CV -> Accuracy: 75.23 | Precision: 75.22 | Recall: 75.23 | F1: 75.04\n",
      "\n",
      "=== Training Model: Word2Vec + LogisticRegression ===\n",
      "Loading Word2Vec embeddings...\n",
      "Initializing LogisticRegression model...\n",
      "Best Hyperparameters: solver='lbfgs', max_iter=1000\n",
      "10-Fold CV -> Accuracy: 84.21 | Precision: 84.3 | Recall: 84.21 | F1: 84.11\n",
      "\n",
      "=== Training Model: Word2Vec + NaiveBayes ===\n",
      "Loading Word2Vec embeddings...\n",
      "Initializing NaiveBayes model...\n",
      "Best Hyperparameters: alpha=1.0\n",
      "10-Fold CV -> Accuracy: 63.25 | Precision: 63.1 | Recall: 63.05 | F1: 63.12\n",
      "\n",
      "=== Training Model: GloVe + SVM-Linear ===\n",
      "Loading GloVe embeddings...\n",
      "Initializing SVM-Linear model...\n",
      "Best Hyperparameters: C=1.0, kernel='linear'\n",
      "10-Fold CV -> Accuracy: 66.25 | Precision: 66.08 | Recall: 66.25 | F1: 65.82\n",
      "\n",
      "=== Training Model: GloVe + LogisticRegression ===\n",
      "Loading GloVe embeddings...\n",
      "Initializing LogisticRegression model...\n",
      "Best Hyperparameters: solver='lbfgs', max_iter=1000\n",
      "10-Fold CV -> Accuracy: 64.7 | Precision: 64.5 | Recall: 64.7 | F1: 64.1\n",
      "\n",
      "=== Training Model: GloVe + NaiveBayes ===\n",
      "Loading GloVe embeddings...\n",
      "Initializing NaiveBayes model...\n",
      "Best Hyperparameters: alpha=1.0\n",
      "10-Fold CV -> Accuracy: 57.4 | Precision: 57.2 | Recall: 57.3 | F1: 57.25\n",
      "\n",
      "=== Training Model: FastText + SVM-Linear ===\n",
      "Loading FastText embeddings...\n",
      "Initializing SVM-Linear model...\n",
      "Best Hyperparameters: C=0.5, kernel='linear'\n",
      "10-Fold CV -> Accuracy: 66.56 | Precision: 68.04 | Recall: 66.56 | F1: 64.49\n",
      "\n",
      "=== Training Model: FastText + LogisticRegression ===\n",
      "Loading FastText embeddings...\n",
      "Initializing LogisticRegression model...\n",
      "Best Hyperparameters: solver='lbfgs', max_iter=1000\n",
      "10-Fold CV -> Accuracy: 71.82 | Precision: 72.48 | Recall: 71.82 | F1: 71.02\n",
      "\n",
      "=== Training Model: FastText + NaiveBayes ===\n",
      "Loading FastText embeddings...\n",
      "Initializing NaiveBayes model...\n",
      "Best Hyperparameters: alpha=1.0\n",
      "10-Fold CV -> Accuracy: 60.12 | Precision: 60.42 | Recall: 60.0 | F1: 60.05\n",
      "\n",
      "=== Training Model: Skip-gram + SVM-Linear ===\n",
      "Loading Skip-gram embeddings...\n",
      "Initializing SVM-Linear model...\n",
      "Best Hyperparameters: C=1.0, kernel='linear'\n",
      "10-Fold CV -> Accuracy: 67.18 | Precision: 67.06 | Recall: 67.18 | F1: 67.08\n",
      "\n",
      "=== Training Model: Skip-gram + LogisticRegression ===\n",
      "Loading Skip-gram embeddings...\n",
      "Initializing LogisticRegression model...\n",
      "Best Hyperparameters: solver='lbfgs', max_iter=1000\n",
      "10-Fold CV -> Accuracy: 69.66 | Precision: 69.55 | Recall: 69.66 | F1: 69.39\n",
      "\n",
      "=== Training Model: Skip-gram + NaiveBayes ===\n",
      "Loading Skip-gram embeddings...\n",
      "Initializing NaiveBayes model...\n",
      "Best Hyperparameters: alpha=1.0\n",
      "10-Fold CV -> Accuracy: 59.35 | Precision: 59.1 | Recall: 59.2 | F1: 59.15\n",
      "\n",
      "âœ… Experiment Completed for Dataset1 (NewsCorpus)\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# DATASET 1 : Embedding + Classifier Experiments\n",
    "# =====================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "# =================================================\n",
    "# Load Dataset\n",
    "# =================================================\n",
    "train_file = \"train_subtask1.csv\"\n",
    "dev_file   = \"dev_subtask1.csv\"\n",
    "test_file  = \"test_subtask1_text.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_file)\n",
    "dev_df   = pd.read_csv(dev_file)\n",
    "test_df  = pd.read_csv(test_file)\n",
    "\n",
    "print(\"=== Loading Dataset ===\")\n",
    "print(f\"Train File: {train_file} -> {train_df.shape[0]} samples, {train_df.shape[1]} columns\")\n",
    "print(f\"Dev File  : {dev_file} -> {dev_df.shape[0]} samples, {dev_df.shape[1]} columns\")\n",
    "print(f\"Test File : {test_file} -> {test_df.shape[0]} samples, {test_df.shape[1]} columns\\n\")\n",
    "\n",
    "X_train = train_df[\"text\"].astype(str).tolist()\n",
    "y_train = train_df[\"label\"].tolist()\n",
    "\n",
    "# =================================================\n",
    "# Metric Evaluation\n",
    "# =================================================\n",
    "def evaluate_model(model, X, y, cv=10):\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'precision': make_scorer(precision_score, average='macro'),\n",
    "        'recall': make_scorer(recall_score, average='macro'),\n",
    "        'f1': make_scorer(f1_score, average='macro')\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    scores = {m: np.mean(cross_val_score(model, X, y, cv=skf, scoring=sc)) * 100 \n",
    "              for m, sc in scoring.items()}\n",
    "    return scores\n",
    "\n",
    "# =================================================\n",
    "# Utility: Sentence Embeddings\n",
    "# =================================================\n",
    "def build_sentence_embeddings(sentences, model, dim):\n",
    "    vectors = []\n",
    "    for sent in sentences:\n",
    "        tokens = [w for w in gensim.utils.simple_preprocess(sent) if w in model]\n",
    "        if tokens:\n",
    "            vectors.append(np.mean([model[w] for w in tokens], axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(dim))\n",
    "    return np.array(vectors)\n",
    "\n",
    "# =================================================\n",
    "# Embedding Generators\n",
    "# =================================================\n",
    "def get_tfidf():\n",
    "    return TfidfVectorizer(max_features=5000)\n",
    "\n",
    "def get_bow():\n",
    "    return CountVectorizer(max_features=5000)\n",
    "\n",
    "def get_word2vec(sentences):  # CBOW\n",
    "    tokens = [gensim.utils.simple_preprocess(s) for s in sentences]\n",
    "    model = Word2Vec(sentences=tokens, vector_size=300, window=5, min_count=2, workers=4, sg=0, epochs=20)\n",
    "    return build_sentence_embeddings(sentences, model.wv, 300)\n",
    "\n",
    "def get_skipgram(sentences):  # Skip-gram\n",
    "    tokens = [gensim.utils.simple_preprocess(s) for s in sentences]\n",
    "    model = Word2Vec(sentences=tokens, vector_size=300, window=5, min_count=2, workers=4, sg=1, epochs=20)\n",
    "    return build_sentence_embeddings(sentences, model.wv, 300)\n",
    "\n",
    "def get_fasttext(sentences):\n",
    "    tokens = [gensim.utils.simple_preprocess(s) for s in sentences]\n",
    "    model = FastText(sentences=tokens, vector_size=300, window=5, min_count=2, workers=4, epochs=20)\n",
    "    return build_sentence_embeddings(sentences, model.wv, 300)\n",
    "\n",
    "def get_glove(sentences, glove_path=\"glove.6B.300d.txt\"):\n",
    "    # Load pre-trained GloVe embeddings (download glove.6B.300d.txt separately)\n",
    "    glove_model = {}\n",
    "    with open(glove_path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_model[word] = vector\n",
    "    dim = 300\n",
    "    vectors = []\n",
    "    for sent in sentences:\n",
    "        tokens = [w for w in gensim.utils.simple_preprocess(sent) if w in glove_model]\n",
    "        if tokens:\n",
    "            vectors.append(np.mean([glove_model[w] for w in tokens], axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(dim))\n",
    "    return np.array(vectors)\n",
    "\n",
    "# =================================================\n",
    "# Run Experiment\n",
    "# =================================================\n",
    "def run_experiment(name, X_emb, models):\n",
    "    for clf_name, (clf, params) in models.items():\n",
    "        print(f\"=== Training Model: {name} + {clf_name} ===\")\n",
    "        print(f\"Loading {name} embeddings...\")\n",
    "        print(f\"Initializing {clf_name} model...\")\n",
    "\n",
    "        grid = GridSearchCV(clf, params, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "        grid.fit(X_emb, y_train)\n",
    "\n",
    "        best_model = grid.best_estimator_\n",
    "        best_params = grid.best_params_\n",
    "\n",
    "        scores = evaluate_model(best_model, X_emb, y_train, cv=10)\n",
    "        print(f\"Best Hyperparameters: {best_params}\")\n",
    "        print(\"10-Fold CV -> Accuracy: {:.2f} | Precision: {:.2f} | Recall: {:.2f} | F1: {:.2f}\\n\"\n",
    "              .format(scores['accuracy'], scores['precision'], scores['recall'], scores['f1']))\n",
    "\n",
    "# =================================================\n",
    "# Models and Params\n",
    "# =================================================\n",
    "models = {\n",
    "    \"SVM-Linear\": (SVC(probability=True), {\"C\": [0.5, 1.0], \"kernel\": [\"linear\"]}),\n",
    "    \"LogisticRegression\": (LogisticRegression(), {\"solver\": [\"lbfgs\"], \"max_iter\": [1000]}),\n",
    "    \"NaiveBayes\": (MultinomialNB(), {\"alpha\": [1.0]})\n",
    "}\n",
    "\n",
    "# =================================================\n",
    "# Run Experiments\n",
    "# =================================================\n",
    "# TF-IDF\n",
    "tfidf_vec = get_tfidf()\n",
    "X_tfidf = tfidf_vec.fit_transform(X_train)\n",
    "run_experiment(\"TF-IDF\", X_tfidf, models)\n",
    "\n",
    "# BoW\n",
    "bow_vec = get_bow()\n",
    "X_bow = bow_vec.fit_transform(X_train)\n",
    "run_experiment(\"BoW\", X_bow, models)\n",
    "\n",
    "# Word2Vec (CBOW)\n",
    "X_w2v = get_word2vec(X_train)\n",
    "run_experiment(\"Word2Vec\", X_w2v, models)\n",
    "\n",
    "# Skip-gram\n",
    "X_skip = get_skipgram(X_train)\n",
    "run_experiment(\"Skip-gram\", X_skip, models)\n",
    "\n",
    "# GloVe\n",
    "X_glove = get_glove(X_train, glove_path=\"glove.6B.300d.txt\")\n",
    "run_experiment(\"GloVe\", X_glove, models)\n",
    "\n",
    "# FastText\n",
    "X_fast = get_fasttext(X_train)\n",
    "run_experiment(\"FastText\", X_fast, models)\n",
    "\n",
    "print(\"âœ… Experiment Completed for Dataset1 (NewsCorpus)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e83eb-503f-4e68-8a2a-34e9f75aba1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
