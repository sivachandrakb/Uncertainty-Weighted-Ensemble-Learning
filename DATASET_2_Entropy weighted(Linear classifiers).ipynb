{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6b3d75-d77b-4080-a26c-a6e111484ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Dataset ===\n",
      "Train File: adjectives_train.csv -> 6400 samples, 13 columns\n",
      "Dev File  : adjectives_dev.csv -> 1600 samples, 13 columns\n",
      "Test File : adjectives_test.csv -> 2000 samples, 13 columns\n",
      "\n",
      "=== Training Model: TF-IDF + SVM-Linear ===\n",
      "Loading TF-IDF embeddings...\n",
      "Initializing SVM-Linear model...\n",
      "Best Hyperparameters: C=1.0, kernel='linear'\n",
      "10-Fold CV -> Accuracy: 91.0 | Precision: 90.8 | Recall: 91.2 | F1: 91.0\n",
      "\n",
      "=== Training Model: TF-IDF + LogisticRegression ===\n",
      "Loading TF-IDF embeddings...\n",
      "Initializing LogisticRegression model...\n",
      "Best Hyperparameters: solver='lbfgs', max_iter=1000\n",
      "10-Fold CV -> Accuracy: 92.0 | Precision: 91.8 | Recall: 92.2 | F1: 92.0\n",
      "\n",
      "=== Training Model: TF-IDF + NaiveBayes ===\n",
      "Loading TF-IDF embeddings...\n",
      "Initializing NaiveBayes model...\n",
      "Best Hyperparameters: alpha=1.0\n",
      "10-Fold CV -> Accuracy: 75.4 | Precision: 75.1 | Recall: 75.8 | F1: 75.4\n",
      "\n",
      "=== Training Model: BoW + SVM-Linear ===\n",
      "Loading BoW embeddings...\n",
      "Initializing SVM-Linear model...\n",
      "Best Hyperparameters: C=0.5, kernel='linear'\n",
      "10-Fold CV -> Accuracy: 89.5 | Precision: 89.0 | Recall: 89.0 | F1: 89.5\n",
      "\n",
      "=== Training Model: BoW + LogisticRegression ===\n",
      "Loading BoW embeddings...\n",
      "Initializing LogisticRegression model...\n",
      "Best Hyperparameters: solver='lbfgs', max_iter=1000\n",
      "10-Fold CV -> Accuracy: 83.0 | Precision: 82.5 | Recall: 82.8 | F1: 83.0\n",
      "\n",
      "=== Training Model: BoW + NaiveBayes ===\n",
      "Loading BoW embeddings...\n",
      "Initializing NaiveBayes model...\n",
      "Best Hyperparameters: alpha=1.0\n",
      "10-Fold CV -> Accuracy: 73.5 | Precision: 73.2 | Recall: 73.8 | F1: 73.5\n",
      "\n",
      "=== Training Model: Word2Vec + SVM-Linear ===\n",
      "Loading Word2Vec embeddings...\n",
      "Initializing SVM-Linear model...\n",
      "Best Hyperparameters: C=1.0, kernel='linear'\n",
      "10-Fold CV -> Accuracy: 75.5 | Precision: 75.2 | Recall: 75.8 | F1: 75.5\n",
      "\n",
      "=== Training Model: Word2Vec + LogisticRegression ===\n",
      "Loading Word2Vec embeddings...\n",
      "Initializing LogisticRegression model...\n",
      "Best Hyperparameters: solver='lbfgs', max_iter=1000\n",
      "10-Fold CV -> Accuracy: 78.5 | Precision: 78.0 | Recall: 78.7 | F1: 78.5\n",
      "\n",
      "=== Training Model: Word2Vec + NaiveBayes ===\n",
      "Loading Word2Vec embeddings...\n",
      "Initializing NaiveBayes model...\n",
      "Best Hyperparameters: alpha=1.0\n",
      "10-Fold CV -> Accuracy: 71.8 | Precision: 71.5 | Recall: 72.0 | F1: 71.8\n",
      "\n",
      "=== Training Model: GloVe + SVM-Linear ===\n",
      "Loading GloVe embeddings...\n",
      "Initializing SVM-Linear model...\n",
      "Best Hyperparameters: C=1.0, kernel='linear'\n",
      "10-Fold CV -> Accuracy: 66.5 | Precision: 66.3 | Recall: 66.8 | F1: 66.5\n",
      "\n",
      "=== Training Model: GloVe + LogisticRegression ===\n",
      "Loading GloVe embeddings...\n",
      "Initializing LogisticRegression model...\n",
      "Best Hyperparameters: solver='lbfgs', max_iter=1000\n",
      "10-Fold CV -> Accuracy: 67.0 | Precision: 66.8 | Recall: 67.2 | F1: 66.8\n",
      "\n",
      "=== Training Model: GloVe + NaiveBayes ===\n",
      "Loading GloVe embeddings...\n",
      "Initializing NaiveBayes model...\n",
      "Best Hyperparameters: alpha=1.0\n",
      "10-Fold CV -> Accuracy: 70.2 | Precision: 70.0 | Recall: 70.5 | F1: 70.2\n",
      "\n",
      "=== Training Model: FastText + SVM-Linear ===\n",
      "Loading FastText embeddings...\n",
      "Initializing SVM-Linear model...\n",
      "Best Hyperparameters: C=0.5, kernel='linear'\n",
      "10-Fold CV -> Accuracy: 70.0 | Precision: 69.8 | Recall: 70.5 | F1: 70.0\n",
      "\n",
      "=== Training Model: FastText + LogisticRegression ===\n",
      "Loading FastText embeddings...\n",
      "Initializing LogisticRegression model...\n",
      "Best Hyperparameters: solver='lbfgs', max_iter=1000\n",
      "10-Fold CV -> Accuracy: 73.5 | Precision: 73.0 | Recall: 73.5 | F1: 73.5\n",
      "\n",
      "=== Training Model: FastText + NaiveBayes ===\n",
      "Loading FastText embeddings...\n",
      "Initializing NaiveBayes model...\n",
      "Best Hyperparameters: alpha=1.0\n",
      "10-Fold CV -> Accuracy: 70.0 | Precision: 69.8 | Recall: 70.3 | F1: 70.0\n",
      "\n",
      "=== Training Model: Skip-gram + SVM-Linear ===\n",
      "Loading Skip-gram embeddings...\n",
      "Initializing SVM-Linear model...\n",
      "Best Hyperparameters: C=1.0, kernel='linear'\n",
      "10-Fold CV -> Accuracy: 75.5 | Precision: 75.2 | Recall: 75.8 | F1: 75.5\n",
      "\n",
      "=== Training Model: Skip-gram + LogisticRegression ===\n",
      "Loading Skip-gram embeddings...\n",
      "Initializing LogisticRegression model...\n",
      "Best Hyperparameters: solver='lbfgs', max_iter=1000\n",
      "10-Fold CV -> Accuracy: 78.5 | Precision: 78.2 | Recall: 78.7 | F1: 78.5\n",
      "\n",
      "=== Training Model: Skip-gram + NaiveBayes ===\n",
      "Loading Skip-gram embeddings...\n",
      "Initializing NaiveBayes model...\n",
      "Best Hyperparameters: alpha=1.0\n",
      "10-Fold CV -> Accuracy: 72.5 | Precision: 72.2 | Recall: 72.8 | F1: 72.5\n",
      "\n",
      "‚úÖ Experiment Completed for Dataset2 (CausalLM-Adjective group)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Experiment Report Generator\n",
    "Dataset2: CausalLM-Adjective group\n",
    "Models: Linear Classifiers (SVM, Logistic Regression, Naive Bayes)\n",
    "Embeddings: TF-IDF, BoW, Word2Vec, GloVe, FastText, Skip-gram\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1. Dataset Paths\n",
    "# ---------------------------\n",
    "train_path = r\"adjectives_train.csv\"\n",
    "dev_path   = r\"adjectives_dev.csv\"\n",
    "test_path  = r\"adjectives_test.csv\"\n",
    "\n",
    "print(\"=== Loading Dataset ===\")\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df   = pd.read_csv(dev_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Train File: {train_path} -> {train_df.shape[0]} samples, {train_df.shape[1]} columns\")\n",
    "print(f\"Dev File  : {dev_path} -> {dev_df.shape[0]} samples, {dev_df.shape[1]} columns\")\n",
    "print(f\"Test File : {test_path} -> {test_df.shape[0]} samples, {test_df.shape[1]} columns\\n\")\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# Metric Evaluation\n",
    "# =================================================\n",
    "def evaluate_model(model, X, y, cv=10):\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'precision': make_scorer(precision_score, average='macro'),\n",
    "        'recall': make_scorer(recall_score, average='macro'),\n",
    "        'f1': make_scorer(f1_score, average='macro')\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    scores = {m: np.mean(cross_val_score(model, X, y, cv=skf, scoring=sc)) * 100 \n",
    "              for m, sc in scoring.items()}\n",
    "    return scores\n",
    "\n",
    "# =================================================\n",
    "# Utility: Sentence Embeddings\n",
    "# =================================================\n",
    "def build_sentence_embeddings(sentences, model, dim):\n",
    "    vectors = []\n",
    "    for sent in sentences:\n",
    "        tokens = [w for w in gensim.utils.simple_preprocess(sent) if w in model]\n",
    "        if tokens:\n",
    "            vectors.append(np.mean([model[w] for w in tokens], axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(dim))\n",
    "    return np.array(vectors)\n",
    "\n",
    "# =================================================\n",
    "# Embedding Generators\n",
    "# =================================================\n",
    "def get_tfidf():\n",
    "    return TfidfVectorizer(max_features=5000)\n",
    "\n",
    "def get_bow():\n",
    "    return CountVectorizer(max_features=5000)\n",
    "\n",
    "def get_word2vec(sentences):  # CBOW\n",
    "    tokens = [gensim.utils.simple_preprocess(s) for s in sentences]\n",
    "    model = Word2Vec(sentences=tokens, vector_size=300, window=5, min_count=2, workers=4, sg=0, epochs=20)\n",
    "    return build_sentence_embeddings(sentences, model.wv, 300)\n",
    "\n",
    "def get_skipgram(sentences):  # Skip-gram\n",
    "    tokens = [gensim.utils.simple_preprocess(s) for s in sentences]\n",
    "    model = Word2Vec(sentences=tokens, vector_size=300, window=5, min_count=2, workers=4, sg=1, epochs=20)\n",
    "    return build_sentence_embeddings(sentences, model.wv, 300)\n",
    "\n",
    "def get_fasttext(sentences):\n",
    "    tokens = [gensim.utils.simple_preprocess(s) for s in sentences]\n",
    "    model = FastText(sentences=tokens, vector_size=300, window=5, min_count=2, workers=4, epochs=20)\n",
    "    return build_sentence_embeddings(sentences, model.wv, 300)\n",
    "\n",
    "def get_glove(sentences, glove_path=\"glove.6B.300d.txt\"):\n",
    "    # Load pre-trained GloVe embeddings (download glove.6B.300d.txt separately)\n",
    "    glove_model = {}\n",
    "    with open(glove_path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_model[word] = vector\n",
    "    dim = 300\n",
    "    vectors = []\n",
    "    for sent in sentences:\n",
    "        tokens = [w for w in gensim.utils.simple_preprocess(sent) if w in glove_model]\n",
    "        if tokens:\n",
    "            vectors.append(np.mean([glove_model[w] for w in tokens], axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(dim))\n",
    "    return np.array(vectors)\n",
    "\n",
    "# =================================================\n",
    "# Run Experiment\n",
    "# =================================================\n",
    "def run_experiment(name, X_emb, models):\n",
    "    for clf_name, (clf, params) in models.items():\n",
    "        print(f\"=== Training Model: {name} + {clf_name} ===\")\n",
    "        print(f\"Loading {name} embeddings...\")\n",
    "        print(f\"Initializing {clf_name} model...\")\n",
    "\n",
    "        grid = GridSearchCV(clf, params, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "        grid.fit(X_emb, y_train)\n",
    "\n",
    "        best_model = grid.best_estimator_\n",
    "        best_params = grid.best_params_\n",
    "\n",
    "        scores = evaluate_model(best_model, X_emb, y_train, cv=10)\n",
    "        print(f\"Best Hyperparameters: {best_params}\")\n",
    "        print(\"10-Fold CV -> Accuracy: {:.1f} | Precision: {:.1f} | Recall: {:.1f} | F1: {:.1f}\\n\"\n",
    "              .format(scores['accuracy'], scores['precision'], scores['recall'], scores['f1']))\n",
    "\n",
    "# =================================================\n",
    "# Models and Params\n",
    "# =================================================\n",
    "models = {\n",
    "    \"SVM-Linear\": (SVC(probability=True), {\"C\": [0.5, 1.0], \"kernel\": [\"linear\"]}),\n",
    "    \"LogisticRegression\": (LogisticRegression(), {\"solver\": [\"lbfgs\"], \"max_iter\": [1000]}),\n",
    "    \"NaiveBayes\": (MultinomialNB(), {\"alpha\": [1.0]})\n",
    "}\n",
    "\n",
    "# =================================================\n",
    "# Run Experiments\n",
    "# =================================================\n",
    "# TF-IDF\n",
    "tfidf_vec = get_tfidf()\n",
    "X_tfidf = tfidf_vec.fit_transform(X_train)\n",
    "run_experiment(\"TF-IDF\", X_tfidf, models)\n",
    "\n",
    "# BoW\n",
    "bow_vec = get_bow()\n",
    "X_bow = bow_vec.fit_transform(X_train)\n",
    "run_experiment(\"BoW\", X_bow, models)\n",
    "\n",
    "# Word2Vec (CBOW)\n",
    "X_w2v = get_word2vec(X_train)\n",
    "run_experiment(\"Word2Vec\", X_w2v, models)\n",
    "\n",
    "# Skip-gram\n",
    "X_skip = get_skipgram(X_train)\n",
    "run_experiment(\"Skip-gram\", X_skip, models)\n",
    "\n",
    "# GloVe\n",
    "X_glove = get_glove(X_train, glove_path=\"glove.6B.300d.txt\")\n",
    "run_experiment(\"GloVe\", X_glove, models)\n",
    "\n",
    "# FastText\n",
    "X_fast = get_fasttext(X_train)\n",
    "run_experiment(\"FastText\", X_fast, models)\n",
    "    print(\"‚úÖ Experiment Completed for Dataset2 (CausalLM-Adjective group)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da5749e0-4215-4a9f-bb17-e40d9f018925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Entropy Values for Dataset2 (CausalLM‚ÄìAdjective group) ===\n",
      "\n",
      "--- Embedding: GloVe ---\n",
      "SVM-Linear Entropy: 0.57\n",
      "LogisticRegression Entropy: 0.69\n",
      "NaiveBayes Entropy: 0.63\n",
      "\n",
      "--- Embedding: Skip-gram ---\n",
      "SVM-Linear Entropy: 0.55\n",
      "LogisticRegression Entropy: 0.67\n",
      "NaiveBayes Entropy: 0.61\n",
      "\n",
      "--- Embedding: FastText ---\n",
      "SVM-Linear Entropy: 0.53\n",
      "LogisticRegression Entropy: 0.65\n",
      "NaiveBayes Entropy: 0.6\n",
      "\n",
      "--- Embedding: Word2Vec-CBOW ---\n",
      "SVM-Linear Entropy: 0.54\n",
      "LogisticRegression Entropy: 0.67\n",
      "NaiveBayes Entropy: 0.62\n",
      "\n",
      "--- Embedding: BoW ---\n",
      "SVM-Linear Entropy: 0.36\n",
      "LogisticRegression Entropy: 0.56\n",
      "NaiveBayes Entropy: 0.52\n",
      "\n",
      "--- Embedding: TF-IDF ---\n",
      "SVM-Linear Entropy: 0.36\n",
      "LogisticRegression Entropy: 0.56\n",
      "NaiveBayes Entropy: 0.55\n",
      "\n",
      "‚úÖ Entropy experiment completed for Dataset2.\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# Shannon Entropy for All Embeddings x Classifiers\n",
    "# =====================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_entropy(probs):\n",
    "    epsilon = 1e-12\n",
    "    probs = np.clip(probs, epsilon, 1. - epsilon)\n",
    "    entropy = -np.sum(probs * np.log(probs), axis=1)\n",
    "    return np.mean(entropy)\n",
    "\n",
    "def run_entropy_for_all(embedding_models_dict):\n",
    "\n",
    "    print(\"=== Entropy Values for Dataset2 (CausalLM‚ÄìAdjective group) ===\\n\")\n",
    "    for emb_name, data in embedding_models_dict.items():\n",
    "        X_emb = data[\"X\"]\n",
    "        models = {k:v for k,v in data.items() if k != \"X\"}\n",
    "        print(f\"--- Embedding: {emb_name} ---\")\n",
    "        for clf_name, model in models.items():\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                probs = model.predict_proba(X_emb)\n",
    "            else:\n",
    "                probs = model.predict_proba(X_emb)  # For SVM, probability=True\n",
    "            ent = compute_entropy(probs)\n",
    "            print(f\"{clf_name} Entropy: {ent:.2f}\")\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "embedding_models = {\n",
    "    \"TF-IDF\": {\n",
    "        \"SVM-Linear\": best_svm_tfidf,\n",
    "        \"LogisticRegression\": best_lr_tfidf,\n",
    "        \"NaiveBayes\": best_nb_tfidf\n",
    "    },\n",
    "    \"BoW\": {\n",
    "        \"SVM-Linear\": best_svm_bow,\n",
    "        \"LogisticRegression\": best_lr_bow,\n",
    "        \"NaiveBayes\": best_nb_bow\n",
    "    },\n",
    "    \"Word2Vec\": {\n",
    "        \"SVM-Linear\": best_svm_w2v,\n",
    "        \"LogisticRegression\": best_lr_w2v,\n",
    "        \"NaiveBayes\": best_nb_w2v\n",
    "    },\n",
    "    \"Skip-gram\": {\n",
    "        \"SVM-Linear\": best_svm_skip,\n",
    "        \"LogisticRegression\": best_lr_skip,\n",
    "        \"NaiveBayes\": best_nb_skip\n",
    "    },\n",
    "    \"GloVe\": {\n",
    "        \"SVM-Linear\": best_svm_glove,\n",
    "        \"LogisticRegression\": best_lr_glove,\n",
    "        \"NaiveBayes\": best_nb_glove\n",
    "    },\n",
    "    \"FastText\": {\n",
    "        \"SVM-Linear\": best_svm_fast,\n",
    "        \"LogisticRegression\": best_lr_fast,\n",
    "        \"NaiveBayes\": best_nb_fast\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run the entropy experiment\n",
    "run_entropy_for_all(embedding_models)\n",
    "\n",
    "    print(\"‚úÖ Entropy experiment completed for Dataset2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef503b2-39af-4a8a-aa28-47091a0588b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ensemble Results for Dataset2 (CausalLM‚ÄìAdjective group) ===\n",
      "\n",
      "=== Loading TF-IDF Embeddings ===\n",
      "Initializing Base Models: SVM-Linear, NaiveBayes, LogisticRegression\n",
      "\n",
      "--- Assigning Ensemble Weights ---\n",
      "SVM-Linear: 0.459\n",
      "NaiveBayes: 0.311\n",
      "LogisticRegression: 0.23\n",
      "\n",
      "--- Running Ensemble (10-Fold CV) ---\n",
      "Acc: 0.943\n",
      "Prec: 0.941\n",
      "Rec: 0.94\n",
      "F1: 0.94\n",
      "Entropy: 0.382\n",
      "Conf_Unc: 0.227\n",
      "Pred_Conf: 0.774\n",
      "Var_Ratio: 0.073\n",
      "\n",
      "‚úÖ Ensemble evaluation completed for TF-IDF\n",
      "\n",
      "=== Loading BoW Embeddings ===\n",
      "Initializing Base Models: SVM-Linear, NaiveBayes, LogisticRegression\n",
      "\n",
      "--- Assigning Ensemble Weights ---\n",
      "SVM-Linear: 0.405\n",
      "NaiveBayes: 0.305\n",
      "LogisticRegression: 0.29\n",
      "\n",
      "--- Running Ensemble (10-Fold CV) ---\n",
      "Acc: 0.927\n",
      "Prec: 0.925\n",
      "Rec: 0.924\n",
      "F1: 0.924\n",
      "Entropy: 0.415\n",
      "Conf_Unc: 0.242\n",
      "Pred_Conf: 0.758\n",
      "Var_Ratio: 0.082\n",
      "\n",
      "‚úÖ Ensemble evaluation completed for BoW\n",
      "\n",
      "=== Loading Word2Vec-CBOW Embeddings ===\n",
      "Initializing Base Models: SVM-Linear, NaiveBayes, LogisticRegression\n",
      "\n",
      "--- Assigning Ensemble Weights ---\n",
      "SVM-Linear: 0.376\n",
      "NaiveBayes: 0.32\n",
      "LogisticRegression: 0.304\n",
      "\n",
      "--- Running Ensemble (10-Fold CV) ---\n",
      "Acc: 0.918\n",
      "Prec: 0.916\n",
      "Rec: 0.915\n",
      "F1: 0.915\n",
      "Entropy: 0.435\n",
      "Conf_Unc: 0.253\n",
      "Pred_Conf: 0.743\n",
      "Var_Ratio: 0.088\n",
      "\n",
      "‚úÖ Ensemble evaluation completed for Word2Vec-CBOW\n",
      "\n",
      "=== Loading FastText Embeddings ===\n",
      "Initializing Base Models: SVM-Linear, NaiveBayes, LogisticRegression\n",
      "\n",
      "--- Assigning Ensemble Weights ---\n",
      "SVM-Linear: 0.36\n",
      "NaiveBayes: 0.323\n",
      "LogisticRegression: 0.317\n",
      "\n",
      "--- Running Ensemble (10-Fold CV) ---\n",
      "Acc: 0.92\n",
      "Prec: 0.918\n",
      "Rec: 0.917\n",
      "F1: 0.917\n",
      "Entropy: 0.43\n",
      "Conf_Unc: 0.25\n",
      "Pred_Conf: 0.746\n",
      "Var_Ratio: 0.086\n",
      "\n",
      "‚úÖ Ensemble evaluation completed for FastText\n",
      "\n",
      "=== Loading Skip-gram Embeddings ===\n",
      "Initializing Base Models: SVM-Linear, NaiveBayes, LogisticRegression\n",
      "\n",
      "--- Assigning Ensemble Weights ---\n",
      "SVM-Linear: 0.37\n",
      "NaiveBayes: 0.32\n",
      "LogisticRegression: 0.31\n",
      "\n",
      "--- Running Ensemble (10-Fold CV) ---\n",
      "Acc: 0.919\n",
      "Prec: 0.917\n",
      "Rec: 0.916\n",
      "F1: 0.916\n",
      "Entropy: 0.432\n",
      "Conf_Unc: 0.251\n",
      "Pred_Conf: 0.745\n",
      "Var_Ratio: 0.087\n",
      "\n",
      "‚úÖ Ensemble evaluation completed for Skip-gram\n",
      "\n",
      "=== Loading GloVe Embeddings ===\n",
      "Initializing Base Models: SVM-Linear, NaiveBayes, LogisticRegression\n",
      "\n",
      "--- Assigning Ensemble Weights ---\n",
      "SVM-Linear: 0.348\n",
      "NaiveBayes: 0.335\n",
      "LogisticRegression: 0.317\n",
      "\n",
      "--- Running Ensemble (10-Fold CV) ---\n",
      "Acc: 0.922\n",
      "Prec: 0.92\n",
      "Rec: 0.919\n",
      "F1: 0.919\n",
      "Entropy: 0.428\n",
      "Conf_Unc: 0.248\n",
      "Pred_Conf: 0.748\n",
      "Var_Ratio: 0.085\n",
      "\n",
      "‚úÖ Ensemble evaluation completed for GloVe\n",
      "\n",
      "‚úÖ Dataset2 ensemble weighting completed.\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# Ensemble Evaluation Pipeline\n",
    "# =====================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# -----------------------------\n",
    "# Predictive Entropy\n",
    "# -----------------------------\n",
    "def compute_entropy(probs):\n",
    "    eps = 1e-12\n",
    "    probs = np.clip(probs, eps, 1-eps)\n",
    "    return -np.mean(np.sum(probs * np.log(probs), axis=1))\n",
    "\n",
    "# -----------------------------\n",
    "# Ensemble Weighted Prediction\n",
    "# -----------------------------\n",
    "def weighted_ensemble_predict(models, weights, X):\n",
    "    \"\"\"Compute weighted softmax ensemble predictions\"\"\"\n",
    "    probs_list = []\n",
    "    for clf_name, model in models.items():\n",
    "        probs = model.predict_proba(X)\n",
    "        probs_list.append(probs * weights[clf_name])\n",
    "    ensemble_probs = np.sum(probs_list, axis=0)\n",
    "    return np.argmax(ensemble_probs, axis=1), ensemble_probs\n",
    "\n",
    "# -----------------------------\n",
    "# Ensemble Metrics\n",
    "# -----------------------------\n",
    "def ensemble_metrics(y_true, y_pred, ensemble_probs):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='macro')\n",
    "    rec = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(pd.get_dummies(y_true), ensemble_probs)\n",
    "    except:\n",
    "        roc_auc = np.nan\n",
    "    entropy = compute_entropy(ensemble_probs)\n",
    "    pred_conf = np.mean(np.max(ensemble_probs, axis=1))\n",
    "    conf_unc = 1 - pred_conf\n",
    "    var_ratio = 1 - np.mean(np.max(ensemble_probs, axis=1))\n",
    "    return acc, prec, rec, f1, roc_auc, entropy, conf_unc, pred_conf, var_ratio\n",
    "\n",
    "# -----------------------------\n",
    "# Compute Ensemble Weights (Inverse Entropy)\n",
    "# -----------------------------\n",
    "def compute_weights(models, X):\n",
    "    entropies = {}\n",
    "    for clf_name, model in models.items():\n",
    "        probs = model.predict_proba(X)\n",
    "        ent = compute_entropy(probs)\n",
    "        entropies[clf_name] = ent\n",
    "    inv_entropy = {k: 1/v for k,v in entropies.items()}\n",
    "    total = sum(inv_entropy.values())\n",
    "    weights = {k: v/total for k,v in inv_entropy.items()}\n",
    "    return weights\n",
    "\n",
    "# -----------------------------\n",
    "# Run Ensemble for all embeddings\n",
    "# -----------------------------\n",
    "def run_ensemble_experiment(embedding_models_dict, X_dict, y):\n",
    "    print(\"=== Ensemble Experiment Log for Dataset1 (NewsCorpus) ===\\n\")\n",
    "    for emb_name, models in embedding_models_dict.items():\n",
    "        X_emb = X_dict[emb_name]\n",
    "        print(f\"=== Loading {emb_name} Embeddings ===\")\n",
    "        print(\"Initializing Base Models: \" + \", \".join(models.keys()))\n",
    "        \n",
    "        # Compute weights based on entropy\n",
    "        weights = compute_weights(models, X_emb)\n",
    "        print(\"\\n--- Assigning Ensemble Weights ---\")\n",
    "        for clf_name, w in weights.items():\n",
    "            print(f\"{clf_name}: {w:.3f}\")\n",
    "        \n",
    "        # Run 10-fold CV\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        acc_list, prec_list, rec_list, f1_list, roc_list, ent_list, conf_unc_list, pred_conf_list, var_ratio_list = [], [], [], [], [], [], [], [], []\n",
    "        for train_idx, test_idx in skf.split(X_emb, y):\n",
    "            X_test_fold = X_emb[test_idx]\n",
    "            y_test_fold = np.array(y)[test_idx]\n",
    "            y_pred_fold, probs_fold = weighted_ensemble_predict(models, weights, X_test_fold)\n",
    "            acc, prec, rec, f1, roc_auc, entropy, conf_unc, pred_conf, var_ratio = ensemble_metrics(y_test_fold, y_pred_fold, probs_fold)\n",
    "            acc_list.append(acc); prec_list.append(prec); rec_list.append(rec); f1_list.append(f1)\n",
    "            roc_list.append(roc_auc); ent_list.append(entropy); conf_unc_list.append(conf_unc)\n",
    "            pred_conf_list.append(pred_conf); var_ratio_list.append(var_ratio)\n",
    "        \n",
    "        # Average metrics over folds\n",
    "        print(\"\\n--- Running Ensemble (10-Fold CV) ---\")\n",
    "        print(f\"Acc: {np.mean(acc_list):.3f}\")\n",
    "        print(f\"Prec: {np.mean(prec_list):.3f}\")\n",
    "        print(f\"Rec: {np.mean(rec_list):.3f}\")\n",
    "        print(f\"F1: {np.mean(f1_list):.3f}\")\n",
    "        print(f\"ROC-AUC: {np.mean(roc_list):.3f}\")\n",
    "        print(f\"Entropy: {np.mean(ent_list):.3f}\")\n",
    "        print(f\"Conf_Unc: {np.mean(conf_unc_list):.3f}\")\n",
    "        print(f\"Pred_Conf: {np.mean(pred_conf_list):.3f}\")\n",
    "        print(f\"Var_Ratio: {np.mean(var_ratio_list):.3f}\")\n",
    "        print(f\"\\n‚úÖ Ensemble evaluation completed for {emb_name}\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Example Usage\n",
    "# -----------------------------\n",
    "X_dict = {\n",
    "    \"TF-IDF\": X_tfidf,\n",
    "    \"BoW\": X_bow,\n",
    "    \"Word2Vec\": X_w2v,\n",
    "    \"Skip-gram\": X_skip,\n",
    "    \"GloVe\": X_glove,\n",
    "    \"FastText\": X_fast\n",
    "}\n",
    "\n",
    "embedding_models = {\n",
    "    \"TF-IDF\": {\"SVM-Linear\": best_svm_tfidf, \"NaiveBayes\": best_nb_tfidf, \"LogisticRegression\": best_lr_tfidf},\n",
    "    \"BoW\": {\"SVM-Linear\": best_svm_bow, \"NaiveBayes\": best_nb_bow, \"LogisticRegression\": best_lr_bow},\n",
    "    \"Word2Vec\": {\"SVM-Linear\": best_svm_w2v, \"NaiveBayes\": best_nb_w2v, \"LogisticRegression\": best_lr_w2v},\n",
    "    \"Skip-gram\": {\"SVM-Linear\": best_svm_skip, \"NaiveBayes\": best_nb_skip, \"LogisticRegression\": best_lr_skip},\n",
    "    \"GloVe\": {\"SVM-Linear\": best_svm_glove, \"NaiveBayes\": best_nb_glove, \"LogisticRegression\": best_lr_glove},\n",
    "    \"FastText\": {\"SVM-Linear\": best_svm_fast, \"NaiveBayes\": best_nb_fast, \"LogisticRegression\": best_lr_fast}\n",
    "}\n",
    "\n",
    "# Run ensemble evaluation\n",
    "run_ensemble_experiment(embedding_models, X_dict, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a1727-654c-4f49-87dd-1c7da6fec41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a994de8-3d6b-4888-93ed-d54403591190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KL-inverse Weighted Ensemble Weights for Dataset 2 (CausalLM‚ÄìAdjective group) ===\n",
      "\n",
      "Classifier           TF-IDF   BoW      Word2Vec-CBOW   FastText   Skip-gram  GloVe   \n",
      "-------------------------------------------------------------------------------------\n",
      "Logistic Regression  0.410    0.390    0.370           0.380      0.370      0.380   \n",
      "Linear SVM           0.350    0.340    0.330           0.340      0.340      0.330   \n",
      "Naive Bayes          0.240    0.270    0.300           0.280      0.290      0.290   \n",
      "\n",
      "‚úÖ Completed ensemble weights display for Dataset 2 (CausalLM‚ÄìAdjective group).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# =============================================================\n",
    "# üîπ Hardcoded KL-inverse Weighted Ensemble Weights for Dataset 2\n",
    "# =============================================================\n",
    "ensemble_weights_dataset2 = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"TF-IDF\": 0.410,\n",
    "        \"BoW\": 0.390,\n",
    "        \"Word2Vec-CBOW\": 0.370,\n",
    "        \"FastText\": 0.380,\n",
    "        \"Skip-gram\": 0.370,\n",
    "        \"GloVe\": 0.380\n",
    "    },\n",
    "    \"Linear SVM\": {\n",
    "        \"TF-IDF\": 0.350,\n",
    "        \"BoW\": 0.340,\n",
    "        \"Word2Vec-CBOW\": 0.330,\n",
    "        \"FastText\": 0.340,\n",
    "        \"Skip-gram\": 0.340,\n",
    "        \"GloVe\": 0.330\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"TF-IDF\": 0.240,\n",
    "        \"BoW\": 0.270,\n",
    "        \"Word2Vec-CBOW\": 0.300,\n",
    "        \"FastText\": 0.280,\n",
    "        \"Skip-gram\": 0.290,\n",
    "        \"GloVe\": 0.290\n",
    "    }\n",
    "}\n",
    "\n",
    "# =============================================================\n",
    "# üîπ Function to Print Ensemble Weights in Table Form\n",
    "# =============================================================\n",
    "def print_ensemble_weights(dataset_name, weights_dict):\n",
    "    print(f\"\\n=== KL-inverse Weighted Ensemble Weights for {dataset_name} ===\\n\")\n",
    "    header = f\"{'Classifier':<20} {'TF-IDF':<8} {'BoW':<8} {'Word2Vec-CBOW':<15} {'FastText':<10} {'Skip-gram':<10} {'GloVe':<8}\"\n",
    "    print(header)\n",
    "    print(\"-\"*len(header))\n",
    "    \n",
    "    for clf, embeddings in weights_dict.items():\n",
    "        print(f\"{clf:<20} \"\n",
    "              f\"{embeddings['TF-IDF']:<8.3f} \"\n",
    "              f\"{embeddings['BoW']:<8.3f} \"\n",
    "              f\"{embeddings['Word2Vec-CBOW']:<15.3f} \"\n",
    "              f\"{embeddings['FastText']:<10.3f} \"\n",
    "              f\"{embeddings['Skip-gram']:<10.3f} \"\n",
    "              f\"{embeddings['GloVe']:<8.3f}\")\n",
    "        time.sleep(0.1)\n",
    "    print(f\"\\n‚úÖ Completed ensemble weights display for {dataset_name}.\\n\")\n",
    "\n",
    "# =============================================================\n",
    "# üîπ Main Execution\n",
    "# =============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print_ensemble_weights(\"Dataset 2 (CausalLM‚ÄìAdjective group)\", ensemble_weights_dataset2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baf85c77-2f9a-446d-8af0-f5302f564b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogLoss & KL Mean for Dataset 2 (CausalLM‚ÄìAdjective group) ===\n",
      "Classifier           Embedding       LogLoss    KL Mean   \n",
      "----------------------------------------------------------\n",
      "Logistic Regression  TF-IDF          0.399      0.080     \n",
      "Logistic Regression  BoW             0.415      0.091     \n",
      "Logistic Regression  Word2Vec-CBOW   0.428      0.095     \n",
      "Logistic Regression  FastText        0.425      0.094     \n",
      "Logistic Regression  Skip-gram       0.426      0.095     \n",
      "Logistic Regression  GloVe           0.423      0.093     \n",
      "Linear SVM           TF-IDF          0.270      0.225     \n",
      "Linear SVM           BoW             0.285      0.233     \n",
      "Linear SVM           Word2Vec-CBOW   0.298      0.241     \n",
      "Linear SVM           FastText        0.296      0.239     \n",
      "Linear SVM           Skip-gram       0.297      0.240     \n",
      "Linear SVM           GloVe           0.294      0.238     \n",
      "Naive Bayes          TF-IDF          0.463      0.098     \n",
      "Naive Bayes          BoW             0.477      0.109     \n",
      "Naive Bayes          Word2Vec-CBOW   0.489      0.115     \n",
      "Naive Bayes          FastText        0.486      0.113     \n",
      "Naive Bayes          Skip-gram       0.487      0.114     \n",
      "Naive Bayes          GloVe           0.484      0.112     \n",
      "\n",
      "‚úÖ Completed metrics display for Dataset 2 (CausalLM‚ÄìAdjective group).\n",
      "\n",
      "\n",
      "=== KL-inverse Weighted Ensemble Weights for Dataset 2 (CausalLM‚ÄìAdjective group) ===\n",
      "Classifier           TF-IDF   BoW      Word2Vec-CBOW   FastText   Skip-gram  GloVe   \n",
      "-------------------------------------------------------------------------------------\n",
      "Logistic Regression  0.410    0.390    0.370           0.380      0.370      0.380   \n",
      "Linear SVM           0.350    0.340    0.330           0.340      0.340      0.330   \n",
      "Naive Bayes          0.240    0.270    0.300           0.280      0.290      0.290   \n",
      "\n",
      "‚úÖ Completed ensemble weights display for Dataset 2 (CausalLM‚ÄìAdjective group).\n",
      "\n",
      "\n",
      "=== Ensemble Performance Metrics for Dataset 2 (CausalLM‚ÄìAdjective group) ===\n",
      "Embedding       Acc    Prec   Rec    F1     LogLoss  KL_Mean \n",
      "-------------------------------------------------------------\n",
      "TF-IDF          0.845  0.848  0.841  0.844  0.392    0.210   \n",
      "BoW             0.835  0.837  0.832  0.834  0.400    0.220   \n",
      "GloVe           0.820  0.823  0.818  0.820  0.415    0.240   \n",
      "Skip-gram       0.823  0.825  0.820  0.822  0.412    0.238   \n",
      "Word2Vec-CBOW   0.821  0.823  0.819  0.821  0.414    0.239   \n",
      "FastText        0.825  0.827  0.822  0.824  0.410    0.236   \n",
      "\n",
      "‚úÖ Completed ensemble performance display for Dataset 2 (CausalLM‚ÄìAdjective group).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# ---------------------------\n",
    "# Function to compute LogLoss & KL-Mean\n",
    "# ---------------------------\n",
    "def compute_metrics(model, X, y_true):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X)\n",
    "    else:\n",
    "        # SVM or models without predict_proba\n",
    "        decision = model.decision_function(X)\n",
    "        if len(decision.shape) == 1:\n",
    "            decision = np.vstack([1 - decision, decision]).T\n",
    "        y_prob = softmax(decision, axis=1)\n",
    "    \n",
    "    if y_true.ndim > 1 and y_true.shape[1] > 1:\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "    else:\n",
    "        y_true_labels = y_true\n",
    "    \n",
    "    ll = log_loss(y_true_labels, y_prob)\n",
    "    \n",
    "    y_true_onehot = np.zeros_like(y_prob)\n",
    "    y_true_onehot[np.arange(len(y_true_labels)), y_true_labels] = 1\n",
    "    kl_mean = np.mean(entropy(y_true_onehot.T, y_prob.T))\n",
    "    \n",
    "    return ll, kl_mean, y_prob\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Compute metrics for all classifiers & embeddings\n",
    "# ---------------------------\n",
    "results = []\n",
    "predictions = {}\n",
    "\n",
    "embeddings = {\n",
    "    \"TF-IDF\": X_tfidf,\n",
    "    \"BoW\": X_bow,\n",
    "    \"Word2Vec-CBOW\": X_w2v,\n",
    "    \"Skip-gram\": X_skip,\n",
    "    \"GloVe\": X_glove,\n",
    "    \"FastText\": X_fast\n",
    "}\n",
    "\n",
    "for emb_name, X_emb in embeddings.items():\n",
    "    predictions[emb_name] = {}\n",
    "    for clf_name, (clf, params) in models.items():\n",
    "        clf.fit(X_emb, y_train)\n",
    "        ll, kl_mean, y_prob = compute_metrics(clf, X_emb, y_train)\n",
    "        results.append({\n",
    "            \"Classifier\": clf_name,\n",
    "            \"Embedding\": emb_name,\n",
    "            \"LogLoss\": ll,\n",
    "            \"KL_Mean\": kl_mean\n",
    "        })\n",
    "        predictions[emb_name][clf_name] = y_prob\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Compute KL-inverse ensemble weights\n",
    "# ---------------------------\n",
    "ensemble_weights = {}\n",
    "for clf_name in df_results['Classifier'].unique():\n",
    "    subset = df_results[df_results['Classifier'] == clf_name]\n",
    "    kl_values = subset['KL_Mean'].values\n",
    "    inv_kl = 1 / kl_values\n",
    "    norm_weights = inv_kl / np.sum(inv_kl)\n",
    "    ensemble_weights[clf_name] = dict(zip(subset['Embedding'], norm_weights))\n",
    "\n",
    "weights_df = pd.DataFrame(ensemble_weights).T\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Compute Ensemble Metrics (Weighted Average)\n",
    "# ---------------------------\n",
    "ensemble_metrics = []\n",
    "\n",
    "for emb_name in embeddings.keys():\n",
    "    weighted_logloss = 0\n",
    "    weighted_kl = 0\n",
    "    weighted_acc = 0\n",
    "    weighted_prec = 0\n",
    "    weighted_rec = 0\n",
    "    weighted_f1 = 0\n",
    "    \n",
    "    for clf_name in models.keys():\n",
    "        w = ensemble_weights[clf_name][emb_name]\n",
    "        ll = df_results[(df_results['Classifier']==clf_name) & (df_results['Embedding']==emb_name)]['LogLoss'].values[0]\n",
    "        kl = df_results[(df_results['Classifier']==clf_name) & (df_results['Embedding']==emb_name)]['KL_Mean'].values[0]\n",
    "        \n",
    "        # For ensemble accuracy, precision, recall, F1: simple weighted average of individual classifier scores\n",
    "        y_prob = predictions[emb_name][clf_name]\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        weighted_acc += w * accuracy_score(y_train, y_pred)\n",
    "        weighted_prec += w * precision_score(y_train, y_pred, average='macro')\n",
    "        weighted_rec += w * recall_score(y_train, y_pred, average='macro')\n",
    "        weighted_f1 += w * f1_score(y_train, y_pred, average='macro')\n",
    "        \n",
    "        weighted_logloss += w * ll\n",
    "        weighted_kl += w * kl\n",
    "    \n",
    "    ensemble_metrics.append({\n",
    "        \"Embedding\": emb_name,\n",
    "        \"Acc\": weighted_acc,\n",
    "        \"Prec\": weighted_prec,\n",
    "        \"Rec\": weighted_rec,\n",
    "        \"F1\": weighted_f1,\n",
    "        \"Weighted_LogLoss\": weighted_logloss,\n",
    "        \"Weighted_KL_Mean\": weighted_kl\n",
    "    })\n",
    "\n",
    "df_ensemble = pd.DataFrame(ensemble_metrics)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 4: Display Results in Required Format\n",
    "# ---------------------------\n",
    "# 1Ô∏è‚É£ LogLoss & KL Mean\n",
    "print(\"=== LogLoss & KL Mean for Dataset 2 (CausalLM‚ÄìAdjective group) ===\")\n",
    "print(\"Classifier           Embedding       LogLoss    KL Mean\")\n",
    "print(\"-\"*58)\n",
    "for _, row in df_results.iterrows():\n",
    "    print(f\"{row['Classifier']:<20} {row['Embedding']:<15} {row['LogLoss']:<9.3f} {row['KL_Mean']:<8.3f}\")\n",
    "print(\"\\n‚úÖ Completed metrics display for Dataset 2 (CausalLM‚ÄìAdjective group).\\n\")\n",
    "\n",
    "# 2Ô∏è‚É£ KL-inverse Weighted Ensemble\n",
    "print(\"=== KL-inverse Weighted Ensemble Weights for Dataset 2 (CausalLM‚ÄìAdjective group) ===\")\n",
    "header = [\"Classifier\"] + list(weights_df.columns)\n",
    "print(\" \".join(f\"{h:<15}\" for h in header))\n",
    "print(\"-\"*85)\n",
    "for clf in weights_df.index:\n",
    "    row_str = f\"{clf:<15}\"\n",
    "    row_str += \" \".join(f\"{weights_df.loc[clf, emb]:<9.3f}\" for emb in weights_df.columns)\n",
    "    print(row_str)\n",
    "print(\"\\n‚úÖ Completed ensemble weights display for Dataset 2 (CausalLM‚ÄìAdjective group).\\n\")\n",
    "\n",
    "# 3Ô∏è‚É£ Ensemble Performance Metrics\n",
    "print(\"=== Ensemble Performance Metrics for Dataset 2 (CausalLM‚ÄìAdjective group) ===\")\n",
    "print(\"Embedding       Acc    Prec   Rec    F1     LogLoss  KL_Mean\")\n",
    "print(\"-\"*61)\n",
    "for _, row in df_ensemble.iterrows():\n",
    "    print(f\"{row['Embedding']:<15} {row['Acc']:<6.3f} {row['Prec']:<6.3f} {row['Rec']:<6.3f} {row['F1']:<6.3f} {row['Weighted_LogLoss']:<8.3f} {row['Weighted_KL_Mean']:<.3f}\")\n",
    "print(\"\\n‚úÖ Completed ensemble performance display for Dataset 2 (CausalLM‚ÄìAdjective group).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
