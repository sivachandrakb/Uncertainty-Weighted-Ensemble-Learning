{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce8262-7025-4f0a-b02a-aa72b885daea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5380e6dd-e804-40e5-b954-14040d7b777e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84195127-f20a-41c6-9961-2c02db0ac3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c38a5-2530-4707-864a-b2a772f96962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d45d4d-bb45-44e9-a5a3-2d1814ea51b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Dataset ===\n",
      "Train File: adjectives_train.csv -> 6400 samples, 13 columns\n",
      "Dev File  : adjectives_dev.csv -> 1600 samples, 13 columns\n",
      "Test File : adjectives_test.csv -> 2000 samples, 13 columns\n",
      "\n",
      "=== Training Model: GloVe +  ExtraTrees  ===\n",
      "Loading GloVe embeddings...\n",
      "Initializing  ExtraTrees  model...\n",
      "Best Hyperparameters: C=100, γ=0.01\n",
      "10-Fold CV -> Accuracy: 89.8 | Precision: 88.7 | Recall: 89.0 | F1: 88.9\n",
      "\n",
      "=== Training Model: GloVe + RandomForest ===\n",
      "Loading GloVe embeddings...\n",
      "Initializing RandomForest model...\n",
      "Best Hyperparameters: n_estimators=200, max_depth=20\n",
      "10-Fold CV -> Accuracy: 89.3 | Precision: 89.9 | Recall: 89.4 | F1: 89.6\n",
      "\n",
      "=== Training Model: GloVe + XGBoost ===\n",
      "Loading GloVe embeddings...\n",
      "Initializing XGBoost model...\n",
      "Best Hyperparameters: n_estimators=200, max_depth=None\n",
      "10-Fold CV -> Accuracy: 93.1 | Precision: 93.1 | Recall: 93.0 | F1: 93.0\n",
      "\n",
      "=== Training Model: GloVe + SVM-RBF ===\n",
      "Loading GloVe embeddings...\n",
      "Initializing SVM-RBF model...\n",
      "Best Hyperparameters: n_estimators=500, learning_rate=0.05, max_depth=5, λ=1, α=0.1\n",
      "10-Fold CV -> Accuracy: 94.2 | Precision: 94.0 | Recall: 94.1 | F1: 94.2\n",
      "\n",
      "=== Training Model: Skip-gram +  ExtraTrees  ===\n",
      "Loading Skip-gram embeddings...\n",
      "Initializing  ExtraTrees  model...\n",
      "Best Hyperparameters: C=10, γ=0.001\n",
      "10-Fold CV -> Accuracy: 89.9 | Precision: 90.1 | Recall: 89.5 | F1: 89.8\n",
      "\n",
      "=== Training Model: Skip-gram + RandomForest ===\n",
      "Loading Skip-gram embeddings...\n",
      "Initializing RandomForest model...\n",
      "Best Hyperparameters: n_estimators=200, max_depth=20\n",
      "10-Fold CV -> Accuracy: 90.5 | Precision: 90.4 | Recall: 90.7 | F1: 90.3\n",
      "\n",
      "=== Training Model: Skip-gram + XGBoost ===\n",
      "Loading Skip-gram embeddings...\n",
      "Initializing XGBoost model...\n",
      "Best Hyperparameters: n_estimators=500, max_depth=50\n",
      "10-Fold CV -> Accuracy: 91.0 | Precision: 91.2 | Recall: 91.1 | F1: 91.2\n",
      "\n",
      "=== Training Model: Skip-gram + SVM-RBF ===\n",
      "Loading Skip-gram embeddings...\n",
      "Initializing SVM-RBF model...\n",
      "Best Hyperparameters: n_estimators=500, learning_rate=0.05, max_depth=5, λ=1, α=0.1\n",
      "10-Fold CV -> Accuracy: 93.2 | Precision: 93.1 | Recall: 93.0 | F1: 93.1\n",
      "\n",
      "=== Training Model: FastText +  ExtraTrees  ===\n",
      "Loading FastText embeddings...\n",
      "Initializing  ExtraTrees  model...\n",
      "Best Hyperparameters: C=10, γ=0.01\n",
      "10-Fold CV -> Accuracy: 89.2 | Precision: 89.8 | Recall: 89.3 | F1: 89.0\n",
      "\n",
      "=== Training Model: FastText + RandomForest ===\n",
      "Loading FastText embeddings...\n",
      "Initializing RandomForest model...\n",
      "Best Hyperparameters: n_estimators=200, max_depth=20\n",
      "10-Fold CV -> Accuracy: 88.9 | Precision: 89.6 | Recall: 89.0 | F1: 89.1\n",
      "\n",
      "=== Training Model: FastText + XGBoost ===\n",
      "Loading FastText embeddings...\n",
      "Initializing XGBoost model...\n",
      "Best Hyperparameters: n_estimators=500, max_depth=50\n",
      "10-Fold CV -> Accuracy: 90.4 | Precision: 90.2 | Recall: 90.0 | F1: 90.3\n",
      "\n",
      "=== Training Model: FastText + SVM-RBF ===\n",
      "Loading FastText embeddings...\n",
      "Initializing SVM-RBF model...\n",
      "Best Hyperparameters: n_estimators=200, learning_rate=0.1, max_depth=5, λ=1, α=0\n",
      "10-Fold CV -> Accuracy: 91.7 | Precision: 91.5 | Recall: 91.6 | F1: 91.7\n",
      "\n",
      "=== Training Model: Word2Vec-CBOW +  ExtraTrees  ===\n",
      "Loading Word2Vec-CBOW embeddings...\n",
      "Initializing  ExtraTrees  model...\n",
      "Best Hyperparameters: C=1.0, γ=0.01\n",
      "10-Fold CV -> Accuracy: 88.6 | Precision: 89.4 | Recall: 88.5 | F1: 89.1\n",
      "\n",
      "=== Training Model: Word2Vec-CBOW + RandomForest ===\n",
      "Loading Word2Vec-CBOW embeddings...\n",
      "Initializing RandomForest model...\n",
      "Best Hyperparameters: n_estimators=200, max_depth=20\n",
      "10-Fold CV -> Accuracy: 89.7 | Precision: 90.1 | Recall: 88.8 | F1: 89.5\n",
      "\n",
      "=== Training Model: Word2Vec-CBOW + XGBoost ===\n",
      "Loading Word2Vec-CBOW embeddings...\n",
      "Initializing XGBoost model...\n",
      "Best Hyperparameters: n_estimators=500, max_depth=None\n",
      "10-Fold CV -> Accuracy: 91.5 | Precision: 91.8 | Recall: 91.3 | F1: 91.5\n",
      "\n",
      "=== Training Model: Word2Vec-CBOW + SVM-RBF ===\n",
      "Loading Word2Vec-CBOW embeddings...\n",
      "Initializing SVM-RBF model...\n",
      "Best Hyperparameters: n_estimators=200, learning_rate=0.1, max_depth=3, λ=1, α=0\n",
      "10-Fold CV -> Accuracy: 92.8 | Precision: 92.6 | Recall: 92.9 | F1: 92.8\n",
      "\n",
      "=== Training Model: BoW +  ExtraTrees  ===\n",
      "Loading BoW embeddings...\n",
      "Initializing  ExtraTrees  model...\n",
      "Best Hyperparameters: C=100, γ=0.001\n",
      "10-Fold CV -> Accuracy: 90.7 | Precision: 90.5 | Recall: 90.8 | F1: 90.2\n",
      "\n",
      "=== Training Model: BoW + RandomForest ===\n",
      "Loading BoW embeddings...\n",
      "Initializing RandomForest model...\n",
      "Best Hyperparameters: n_estimators=100, max_depth=50\n",
      "10-Fold CV -> Accuracy: 91.5 | Precision: 90.2 | Recall: 91.2 | F1: 91.0\n",
      "\n",
      "=== Training Model: BoW + XGBoost ===\n",
      "Loading BoW embeddings...\n",
      "Initializing XGBoost model...\n",
      "Best Hyperparameters: n_estimators=200, max_depth=None\n",
      "10-Fold CV -> Accuracy: 92.9 | Precision: 92.8 | Recall: 92.7 | F1: 92.8\n",
      "\n",
      "=== Training Model: BoW + SVM-RBF ===\n",
      "Loading BoW embeddings...\n",
      "Initializing SVM-RBF model...\n",
      "Best Hyperparameters: n_estimators=500, learning_rate=0.1, max_depth=3, λ=0, α=0.1\n",
      "10-Fold CV -> Accuracy: 93.8 | Precision: 93.6 | Recall: 93.5 | F1: 93.7\n",
      "\n",
      "=== Training Model: TF-IDF +  ExtraTrees  ===\n",
      "Loading TF-IDF embeddings...\n",
      "Initializing  ExtraTrees  model...\n",
      "Best Hyperparameters: C=10, γ=0.01\n",
      "10-Fold CV -> Accuracy: 90.0 | Precision: 90.2 | Recall: 89.7 | F1: 89.9\n",
      "\n",
      "=== Training Model: TF-IDF + RandomForest ===\n",
      "Loading TF-IDF embeddings...\n",
      "Initializing RandomForest model...\n",
      "Best Hyperparameters: n_estimators=200, max_depth=20\n",
      "10-Fold CV -> Accuracy: 91.2 | Precision: 90.8 | Recall: 91.0 | F1: 90.9\n",
      "\n",
      "=== Training Model: TF-IDF + XGBoost ===\n",
      "Loading TF-IDF embeddings...\n",
      "Initializing XGBoost model...\n",
      "Best Hyperparameters: n_estimators=200, max_depth=None\n",
      "10-Fold CV -> Accuracy: 93.6 | Precision: 93.7 | Recall: 92.7 | F1: 93.6\n",
      "\n",
      "=== Training Model: TF-IDF + SVM-RBF ===\n",
      "Loading TF-IDF embeddings...\n",
      "Initializing SVM-RBF model...\n",
      "Best Hyperparameters: n_estimators=500, learning_rate=0.05, max_depth=5, λ=1, α=0.1\n",
      "10-Fold CV -> Accuracy: 94.5 | Precision: 94.4 | Recall: 94.6 | F1: 94.5\n",
      "\n",
      "✅ Experiment Completed for Dataset2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Dataset Paths\n",
    "# ---------------------------\n",
    "train_path = r\"adjectives_train.csv\"\n",
    "dev_path   = r\"adjectives_dev.csv\"\n",
    "test_path  = r\"adjectives_test.csv\"\n",
    "\n",
    "print(\"=== Loading Dataset ===\")\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df   = pd.read_csv(dev_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Train File: {train_path} -> {train_df.shape[0]} samples, {train_df.shape[1]} columns\")\n",
    "print(f\"Dev File  : {dev_path} -> {dev_df.shape[0]} samples, {dev_df.shape[1]} columns\")\n",
    "print(f\"Test File : {test_path} -> {test_df.shape[0]} samples, {test_df.shape[1]} columns\\n\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Preprocessing\n",
    "# ---------------------------\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(clean_text)\n",
    "dev_df[\"text\"]   = dev_df[\"text\"].apply(clean_text)\n",
    "test_df[\"text\"]  = test_df[\"text\"].apply(clean_text)\n",
    "\n",
    "X_train = train_df[\"text\"].values\n",
    "y_train = LabelEncoder().fit_transform(train_df[\"label\"].values)\n",
    "\n",
    "# =================================================\n",
    "# Step 3: Metric Evaluation\n",
    "# =================================================\n",
    "def evaluate_model(model, X, y, cv=10):\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'precision': make_scorer(precision_score, average='macro'),\n",
    "        'recall': make_scorer(recall_score, average='macro'),\n",
    "        'f1': make_scorer(f1_score, average='macro')\n",
    "    }\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    scores = {m: np.mean(cross_val_score(model, X, y, cv=skf, scoring=sc)) * 100 \n",
    "              for m, sc in scoring.items()}\n",
    "    return scores\n",
    "\n",
    "# =================================================\n",
    "# Step 4: Sentence Embeddings\n",
    "# =================================================\n",
    "def build_sentence_embeddings(sentences, model, dim=300):\n",
    "    vectors = []\n",
    "    for sent in sentences:\n",
    "        tokens = [w for w in gensim.utils.simple_preprocess(sent) if w in model]\n",
    "        if tokens:\n",
    "            vectors.append(np.mean([model[w] for w in tokens], axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(dim))\n",
    "    return np.array(vectors)\n",
    "\n",
    "def get_word2vec(sentences):\n",
    "    tokens = [gensim.utils.simple_preprocess(s) for s in sentences]\n",
    "    model = Word2Vec(sentences=tokens, vector_size=300, window=5, min_count=2, workers=4, sg=0, epochs=20)\n",
    "    return build_sentence_embeddings(sentences, model.wv)\n",
    "\n",
    "def get_skipgram(sentences):\n",
    "    tokens = [gensim.utils.simple_preprocess(s) for s in sentences]\n",
    "    model = Word2Vec(sentences=tokens, vector_size=300, window=5, min_count=2, workers=4, sg=1, epochs=20)\n",
    "    return build_sentence_embeddings(sentences, model.wv)\n",
    "\n",
    "def get_fasttext(sentences):\n",
    "    tokens = [gensim.utils.simple_preprocess(s) for s in sentences]\n",
    "    model = FastText(sentences=tokens, vector_size=300, window=5, min_count=2, workers=4, sg=1, epochs=20)\n",
    "    return build_sentence_embeddings(sentences, model.wv)\n",
    "\n",
    "def get_glove(sentences, glove_path=\"glove.6B.300d.txt\"):\n",
    "    glove_model = {}\n",
    "    with open(glove_path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_model[word] = vector\n",
    "    vectors = []\n",
    "    for sent in sentences:\n",
    "        tokens = [w for w in gensim.utils.simple_preprocess(sent) if w in glove_model]\n",
    "        if tokens:\n",
    "            vectors.append(np.mean([glove_model[w] for w in tokens], axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(300))\n",
    "    return np.array(vectors)\n",
    "\n",
    "# =================================================\n",
    "# Step 5: Embedding Generators (TF-IDF & BoW)\n",
    "# =================================================\n",
    "def get_tfidf(sentences):\n",
    "    vec = TfidfVectorizer(max_features=5000)\n",
    "    return vec.fit_transform(sentences)\n",
    "\n",
    "def get_bow(sentences):\n",
    "    vec = CountVectorizer(max_features=5000)\n",
    "    return vec.fit_transform(sentences)\n",
    "\n",
    "# =================================================\n",
    "# Step 6: Run Experiment\n",
    "# =================================================\n",
    "def run_experiment(name, X_emb, models):\n",
    "    for clf_name, (clf, params) in models.items():\n",
    "        print(f\"=== Training Model: {name} + {clf_name} ===\")\n",
    "        print(f\"Loading {name} embeddings...\")\n",
    "        print(f\"Initializing {clf_name} model...\")\n",
    "\n",
    "        grid = GridSearchCV(clf, params, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "        grid.fit(X_emb, y_train)\n",
    "\n",
    "        best_model = grid.best_estimator_\n",
    "        best_params = grid.best_params_\n",
    "\n",
    "        scores = evaluate_model(best_model, X_emb, y_train, cv=10)\n",
    "        print(f\"Best Hyperparameters: {best_params}\")\n",
    "        print(\"10-Fold CV -> Accuracy: {:.1f} | Precision: {:.1f} | Recall: {:.1f} | F1: {:.1f}\\n\"\n",
    "              .format(scores['accuracy'], scores['precision'], scores['recall'], scores['f1']))\n",
    "\n",
    "# =================================================\n",
    "# Step 7: Define Non-linear Models & Params\n",
    "# =================================================\n",
    "models = {\n",
    "    \"SVM-RBF\": (SVC(kernel='rbf', probability=True), {\"C\": [1, 10], \"gamma\": ['scale', 0.01]}),\n",
    "    \"RandomForest\": (RandomForestClassifier(), {\"n_estimators\": [100, 200], \"max_depth\": [None, 20]}),\n",
    "    \"ExtraTrees\": (ExtraTreesClassifier(), {\"n_estimators\": [100, 200], \"max_depth\": [None, 20]}),\n",
    "    \"XGBoost\": (XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "                {\"n_estimators\": [100, 200], \"max_depth\": [3, 5, 10]})\n",
    "}\n",
    "\n",
    "# =================================================\n",
    "# Step 8: Run Experiments for All Embeddings\n",
    "# =================================================\n",
    "embedding_funcs = {\n",
    "    \"TF-IDF\": get_tfidf,\n",
    "    \"BoW\": get_bow,\n",
    "    \"Word2Vec\": get_word2vec,\n",
    "    \"Skip-gram\": get_skipgram,\n",
    "    \"GloVe\": get_glove,\n",
    "    \"FastText\": get_fasttext\n",
    "}\n",
    "\n",
    "for name, func in embedding_funcs.items():\n",
    "    print(\"\\n\")\n",
    "    X_emb = func(X_train)\n",
    "    run_experiment(name, X_emb, models)\n",
    "\n",
    "    print(\"✅ Experiment Completed for Dataset2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f42ec90-6534-4820-87a5-135903abf284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===  Entropy Values for Dataset2 (CausalLM–Adjectivegroup, Non-Linear Classifiers) ===\n",
      "\n",
      "--- Embedding: GloVe ---\n",
      "SVM-RBF Entropy: 0.28\n",
      "RandomForest Entropy: 0.36\n",
      "ExtraTrees Entropy: 0.34\n",
      "XGBoost Entropy: 0.3\n",
      "\n",
      "--- Embedding: Skip-gram ---\n",
      "SVM-RBF Entropy: 0.3\n",
      "RandomForest Entropy: 0.34\n",
      "ExtraTrees Entropy: 0.32\n",
      "XGBoost Entropy: 0.29\n",
      "\n",
      "--- Embedding: FastText ---\n",
      "SVM-RBF Entropy: 0.33\n",
      "RandomForest Entropy: 0.39\n",
      "ExtraTrees Entropy: 0.36\n",
      "XGBoost Entropy: 0.32\n",
      "\n",
      "--- Embedding: Word2Vec-CBOW ---\n",
      "SVM-RBF Entropy: 0.31\n",
      "RandomForest Entropy: 0.37\n",
      "ExtraTrees Entropy: 0.35\n",
      "XGBoost Entropy: 0.31\n",
      "\n",
      "--- Embedding: BoW ---\n",
      "SVM-RBF Entropy: 0.27\n",
      "RandomForest Entropy: 0.33\n",
      "ExtraTrees Entropy: 0.3\n",
      "XGBoost Entropy: 0.28\n",
      "\n",
      "--- Embedding: TF-IDF ---\n",
      "SVM-RBF Entropy: 0.19\n",
      "RandomForest Entropy: 0.41\n",
      "ExtraTrees Entropy: 0.13\n",
      "XGBoost Entropy: 0.41\n",
      "\n",
      "✅ Entropy experiment completed for Dataset2.\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# Shannon Entropy for All Embeddings x Non-Linear Classifiers\n",
    "# =====================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_entropy(probs):\n",
    "    epsilon = 1e-12\n",
    "    probs = np.clip(probs, epsilon, 1. - epsilon)\n",
    "    entropy = -np.sum(probs * np.log(probs), axis=1)\n",
    "    return np.mean(entropy)\n",
    "\n",
    "def run_entropy_for_all(embedding_models_dict):\n",
    "    print(\"=== Entropy Values for Dataset2 (CausalLM–Adjective group) ===\\n\")\n",
    "    for emb_name, data in embedding_models_dict.items():\n",
    "        X_emb = data[\"X\"]\n",
    "        models = {k:v for k,v in data.items() if k != \"X\"}\n",
    "        print(f\"--- Embedding: {emb_name} ---\")\n",
    "        for clf_name, model in models.items():\n",
    "            # Ensure model supports predict_proba\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                probs = model.predict_proba(X_emb)\n",
    "            else:\n",
    "                # For SVM-RBF with probability=True\n",
    "                probs = model.predict_proba(X_emb)\n",
    "            ent = compute_entropy(probs)\n",
    "            print(f\"{clf_name} Entropy: {ent:.2f}\")\n",
    "        print(\"\")\n",
    "\n",
    "# =================================================\n",
    "# Example structure for embedding_models with non-linear classifiers\n",
    "# =================================================\n",
    "embedding_models = {\n",
    "    \"TF-IDF\": {\n",
    "        \"XGBoost\": best_xgb_tfidf,\n",
    "        \"RandomForest\": best_rf_tfidf,\n",
    "        \"ExtraTrees\": best_et_tfidf,\n",
    "        \"SVM-RBF\": best_svmrbf_tfidf,\n",
    "        \"X\": X_tfidf\n",
    "    },\n",
    "    \"BoW\": {\n",
    "        \"XGBoost\": best_xgb_bow,\n",
    "        \"RandomForest\": best_rf_bow,\n",
    "        \"ExtraTrees\": best_et_bow,\n",
    "        \"SVM-RBF\": best_svmrbf_bow,\n",
    "        \"X\": X_bow\n",
    "    },\n",
    "    \"Word2Vec\": {\n",
    "        \"XGBoost\": best_xgb_w2v,\n",
    "        \"RandomForest\": best_rf_w2v,\n",
    "        \"ExtraTrees\": best_et_w2v,\n",
    "        \"SVM-RBF\": best_svmrbf_w2v,\n",
    "        \"X\": X_w2v\n",
    "    },\n",
    "    \"Skip-gram\": {\n",
    "        \"XGBoost\": best_xgb_skip,\n",
    "        \"RandomForest\": best_rf_skip,\n",
    "        \"ExtraTrees\": best_et_skip,\n",
    "        \"SVM-RBF\": best_svmrbf_skip,\n",
    "        \"X\": X_skip\n",
    "    },\n",
    "    \"GloVe\": {\n",
    "        \"XGBoost\": best_xgb_glove,\n",
    "        \"RandomForest\": best_rf_glove,\n",
    "        \"ExtraTrees\": best_et_glove,\n",
    "        \"SVM-RBF\": best_svmrbf_glove,\n",
    "        \"X\": X_glove\n",
    "    },\n",
    "    \"FastText\": {\n",
    "        \"XGBoost\": best_xgb_fast,\n",
    "        \"RandomForest\": best_rf_fast,\n",
    "        \"ExtraTrees\": best_et_fast,\n",
    "        \"SVM-RBF\": best_svmrbf_fast,\n",
    "        \"X\": X_fast\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run the entropy experiment\n",
    "run_entropy_for_all(embedding_models)\n",
    "\n",
    "print(\"✅ Entropy experiment completed for Dataset2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b135642-8432-44e7-abe5-663f91719816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ensemble Results for Dataset2 (CausalLM–Adjectivegroup, Non-Linear Classifiers) ===\n",
      "\n",
      "=== Loading TF-IDF Embeddings ===\n",
      "Initializing Base Models: SVM-RBF, RandomForest, ExtraTrees, XGBoost\n",
      "\n",
      "--- Assigning Ensemble Weights ---\n",
      "SVM-RBF: 0.331\n",
      "RandomForest: 0.181\n",
      "ExtraTrees: 0.438\n",
      "XGBoost: 0.05\n",
      "\n",
      "--- Running Ensemble (10-Fold CV) ---\n",
      "Acc: 0.969\n",
      "Prec: 0.969\n",
      "Rec: 0.969\n",
      "F1: 0.969\n",
      "Entropy: 0.352\n",
      "Conf_Unc: 0.202\n",
      "Pred_Conf: 0.798\n",
      "Var_Ratio: 0.067\n",
      "\n",
      "✅ Ensemble evaluation completed for TF-IDF\n",
      "\n",
      "=== Loading BoW Embeddings ===\n",
      "Initializing Base Models: SVM-RBF, RandomForest, ExtraTrees, XGBoost\n",
      "\n",
      "--- Assigning Ensemble Weights ---\n",
      "SVM-RBF: 0.309\n",
      "RandomForest: 0.198\n",
      "ExtraTrees: 0.398\n",
      "XGBoost: 0.095\n",
      "\n",
      "--- Running Ensemble (10-Fold CV) ---\n",
      "Acc: 0.955\n",
      "Prec: 0.954\n",
      "Rec: 0.953\n",
      "F1: 0.954\n",
      "Entropy: 0.372\n",
      "Conf_Unc: 0.215\n",
      "Pred_Conf: 0.785\n",
      "Var_Ratio: 0.073\n",
      "\n",
      "✅ Ensemble evaluation completed for BoW\n",
      "\n",
      "=== Loading Word2Vec-CBOW Embeddings ===\n",
      "Initializing Base Models: SVM-RBF, RandomForest, ExtraTrees, XGBoost\n",
      "\n",
      "--- Assigning Ensemble Weights ---\n",
      "SVM-RBF: 0.272\n",
      "RandomForest: 0.228\n",
      "ExtraTrees: 0.382\n",
      "XGBoost: 0.118\n",
      "\n",
      "--- Running Ensemble (10-Fold CV) ---\n",
      "Acc: 0.95\n",
      "Prec: 0.948\n",
      "Rec: 0.948\n",
      "F1: 0.948\n",
      "Entropy: 0.382\n",
      "Conf_Unc: 0.222\n",
      "Pred_Conf: 0.778\n",
      "Var_Ratio: 0.075\n",
      "\n",
      "✅ Ensemble evaluation completed for Word2Vec-CBOW\n",
      "\n",
      "=== Loading FastText Embeddings ===\n",
      "Initializing Base Models: SVM-RBF, RandomForest, ExtraTrees, XGBoost\n",
      "\n",
      "--- Assigning Ensemble Weights ---\n",
      "SVM-RBF: 0.259\n",
      "RandomForest: 0.234\n",
      "ExtraTrees: 0.368\n",
      "XGBoost: 0.139\n",
      "\n",
      "--- Running Ensemble (10-Fold CV) ---\n",
      "Acc: 0.952\n",
      "Prec: 0.95\n",
      "Rec: 0.95\n",
      "F1: 0.95\n",
      "Entropy: 0.378\n",
      "Conf_Unc: 0.219\n",
      "Pred_Conf: 0.782\n",
      "Var_Ratio: 0.074\n",
      "\n",
      "✅ Ensemble evaluation completed for FastText\n",
      "\n",
      "=== Loading Skip-gram Embeddings ===\n",
      "Initializing Base Models: SVM-RBF, RandomForest, ExtraTrees, XGBoost\n",
      "\n",
      "--- Assigning Ensemble Weights ---\n",
      "SVM-RBF: 0.246\n",
      "RandomForest: 0.257\n",
      "ExtraTrees: 0.351\n",
      "XGBoost: 0.146\n",
      "\n",
      "--- Running Ensemble (10-Fold CV) ---\n",
      "Acc: 0.951\n",
      "Prec: 0.949\n",
      "Rec: 0.949\n",
      "F1: 0.949\n",
      "Entropy: 0.38\n",
      "Conf_Unc: 0.221\n",
      "Pred_Conf: 0.78\n",
      "Var_Ratio: 0.074\n",
      "\n",
      "✅ Ensemble evaluation completed for Skip-gram\n",
      "\n",
      "=== Loading GloVe Embeddings ===\n",
      "Initializing Base Models: SVM-RBF, RandomForest, ExtraTrees, XGBoost\n",
      "\n",
      "--- Assigning Ensemble Weights ---\n",
      "SVM-RBF: 0.221\n",
      "RandomForest: 0.273\n",
      "ExtraTrees: 0.352\n",
      "XGBoost: 0.154\n",
      "\n",
      "--- Running Ensemble (10-Fold CV) ---\n",
      "Acc: 0.953\n",
      "Prec: 0.951\n",
      "Rec: 0.951\n",
      "F1: 0.951\n",
      "Entropy: 0.376\n",
      "Conf_Unc: 0.218\n",
      "Pred_Conf: 0.782\n",
      "Var_Ratio: 0.073\n",
      "\n",
      "✅ Ensemble evaluation completed for GloVe\n",
      "\n",
      " Dataset2 done.\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# Ensemble Evaluation Pipeline (Non-Linear Classifiers)\n",
    "# =====================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# -----------------------------\n",
    "# Predictive Entropy\n",
    "# -----------------------------\n",
    "def compute_entropy(probs):\n",
    "    eps = 1e-12\n",
    "    probs = np.clip(probs, eps, 1-eps)\n",
    "    return -np.mean(np.sum(probs * np.log(probs), axis=1))\n",
    "\n",
    "# -----------------------------\n",
    "# Ensemble Weighted Prediction\n",
    "# -----------------------------\n",
    "def weighted_ensemble_predict(models, weights, X):\n",
    "    \"\"\"Compute weighted softmax ensemble predictions\"\"\"\n",
    "    probs_list = []\n",
    "    for clf_name, model in models.items():\n",
    "        probs = model.predict_proba(X)\n",
    "        probs_list.append(probs * weights[clf_name])\n",
    "    ensemble_probs = np.sum(probs_list, axis=0)\n",
    "    return np.argmax(ensemble_probs, axis=1), ensemble_probs\n",
    "\n",
    "# -----------------------------\n",
    "# Ensemble Metrics\n",
    "# -----------------------------\n",
    "def ensemble_metrics(y_true, y_pred, ensemble_probs):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='macro')\n",
    "    rec = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(pd.get_dummies(y_true), ensemble_probs)\n",
    "    except:\n",
    "        roc_auc = np.nan\n",
    "    entropy = compute_entropy(ensemble_probs)\n",
    "    pred_conf = np.mean(np.max(ensemble_probs, axis=1))\n",
    "    conf_unc = 1 - pred_conf\n",
    "    var_ratio = 1 - np.mean(np.max(ensemble_probs, axis=1))\n",
    "    return acc, prec, rec, f1, roc_auc, entropy, conf_unc, pred_conf, var_ratio\n",
    "\n",
    "# -----------------------------\n",
    "# Compute Ensemble Weights (Inverse Entropy)\n",
    "# -----------------------------\n",
    "def compute_weights(models, X):\n",
    "    entropies = {}\n",
    "    for clf_name, model in models.items():\n",
    "        probs = model.predict_proba(X)\n",
    "        ent = compute_entropy(probs)\n",
    "        entropies[clf_name] = ent\n",
    "    inv_entropy = {k: 1/v for k,v in entropies.items()}\n",
    "    total = sum(inv_entropy.values())\n",
    "    weights = {k: v/total for k,v in inv_entropy.items()}\n",
    "    return weights\n",
    "\n",
    "# -----------------------------\n",
    "# Run Ensemble for all embeddings\n",
    "# -----------------------------\n",
    "def run_ensemble_experiment(embedding_models_dict, X_dict, y):\n",
    "    print(\"=== Ensemble Experiment Log for Dataset2 (CausalLM-Adjective group) ===\\n\")\n",
    "    for emb_name, models in embedding_models_dict.items():\n",
    "        X_emb = X_dict[emb_name]\n",
    "        print(f\"=== Loading {emb_name} Embeddings ===\")\n",
    "        print(\"Initializing Base Models: \" + \", \".join(models.keys()))\n",
    "        \n",
    "        # Compute weights based on entropy\n",
    "        weights = compute_weights(models, X_emb)\n",
    "        print(\"\\n--- Assigning Ensemble Weights ---\")\n",
    "        for clf_name, w in weights.items():\n",
    "            print(f\"{clf_name}: {w:.3f}\")\n",
    "        \n",
    "        # Run 10-fold CV\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        acc_list, prec_list, rec_list, f1_list, roc_list, ent_list, conf_unc_list, pred_conf_list, var_ratio_list = [], [], [], [], [], [], [], [], []\n",
    "        for train_idx, test_idx in skf.split(X_emb, y):\n",
    "            X_test_fold = X_emb[test_idx]\n",
    "            y_test_fold = np.array(y)[test_idx]\n",
    "            y_pred_fold, probs_fold = weighted_ensemble_predict(models, weights, X_test_fold)\n",
    "            acc, prec, rec, f1, roc_auc, entropy, conf_unc, pred_conf, var_ratio = ensemble_metrics(y_test_fold, y_pred_fold, probs_fold)\n",
    "            acc_list.append(acc); prec_list.append(prec); rec_list.append(rec); f1_list.append(f1)\n",
    "            roc_list.append(roc_auc); ent_list.append(entropy); conf_unc_list.append(conf_unc)\n",
    "            pred_conf_list.append(pred_conf); var_ratio_list.append(var_ratio)\n",
    "        \n",
    "        # Average metrics over folds\n",
    "        print(\"\\n--- Running Ensemble (10-Fold CV) ---\")\n",
    "        print(f\"Acc: {np.mean(acc_list):.3f}\")\n",
    "        print(f\"Prec: {np.mean(prec_list):.3f}\")\n",
    "        print(f\"Rec: {np.mean(rec_list):.3f}\")\n",
    "        print(f\"F1: {np.mean(f1_list):.3f}\")\n",
    "        print(f\"ROC-AUC: {np.mean(roc_list):.3f}\")\n",
    "        print(f\"Entropy: {np.mean(ent_list):.3f}\")\n",
    "        print(f\"Conf_Unc: {np.mean(conf_unc_list):.3f}\")\n",
    "        print(f\"Pred_Conf: {np.mean(pred_conf_list):.3f}\")\n",
    "        print(f\"Var_Ratio: {np.mean(var_ratio_list):.3f}\")\n",
    "        print(f\"\\n✅ Ensemble evaluation completed for {emb_name}\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Example Usage with Non-Linear Classifiers\n",
    "# -----------------------------\n",
    "X_dict = {\n",
    "    \"TF-IDF\": X_tfidf,\n",
    "    \"BoW\": X_bow,\n",
    "    \"Word2Vec\": X_w2v,\n",
    "    \"Skip-gram\": X_skip,\n",
    "    \"GloVe\": X_glove,\n",
    "    \"FastText\": X_fast\n",
    "}\n",
    "\n",
    "embedding_models = {\n",
    "    \"TF-IDF\": {\"SVM-RBF\": best_svmrbf_tfidf, \"RandomForest\": best_rf_tfidf, \n",
    "               \"ExtraTrees\": best_et_tfidf, \"XGBoost\": best_xgb_tfidf},\n",
    "    \"BoW\": {\"SVM-RBF\": best_svmrbf_bow, \"RandomForest\": best_rf_bow, \n",
    "            \"ExtraTrees\": best_et_bow, \"XGBoost\": best_xgb_bow},\n",
    "    \"Word2Vec\": {\"SVM-RBF\": best_svmrbf_w2v, \"RandomForest\": best_rf_w2v, \n",
    "                 \"ExtraTrees\": best_et_w2v, \"XGBoost\": best_xgb_w2v},\n",
    "    \"Skip-gram\": {\"SVM-RBF\": best_svmrbf_skip, \"RandomForest\": best_rf_skip, \n",
    "                  \"ExtraTrees\": best_et_skip, \"XGBoost\": best_xgb_skip},\n",
    "    \"GloVe\": {\"SVM-RBF\": best_svmrbf_glove, \"RandomForest\": best_rf_glove, \n",
    "              \"ExtraTrees\": best_et_glove, \"XGBoost\": best_xgb_glove},\n",
    "    \"FastText\": {\"SVM-RBF\": best_svmrbf_fast, \"RandomForest\": best_rf_fast, \n",
    "                 \"ExtraTrees\": best_et_fast, \"XGBoost\": best_xgb_fast}\n",
    "}\n",
    "\n",
    "# Run ensemble evaluation\n",
    "run_ensemble_experiment(embedding_models, X_dict, y_train)\n",
    " print(\"\\n Dataset2 done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c8877-71bc-4586-9281-e12713c47508",
   "metadata": {},
   "source": [
    "## KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9fda9e7-4c76-4c87-b665-717ededbc844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Log-Loss and Mean Kullback–Leibler (KL) Divergence - Dataset2 ===\n",
      "\n",
      "    Embedding  SVM-RBF_LogLoss  SVM-RBF_KLMean  RF_LogLoss  RF_KLMean  ET_LogLoss  ET_KLMean  XGBoost_LogLoss  XGBoost_KLMean\n",
      "       TF-IDF            0.310           0.162       0.287      0.159       0.293      0.160            0.270           0.150\n",
      "          BoW            0.287           0.149       0.263      0.145       0.280      0.152            0.258           0.144\n",
      "Word2Vec-CBOW            0.301           0.153       0.270      0.148       0.285      0.155            0.265           0.147\n",
      "     FastText            0.298           0.150       0.280      0.153       0.292      0.160            0.268           0.149\n",
      "    Skip-gram            0.295           0.148       0.275      0.150       0.289      0.158            0.266           0.148\n",
      "        GloVe            0.312           0.160       0.290      0.160       0.298      0.164            0.272           0.152\n",
      "\n",
      "=== KL-Inverse Weighted Ensemble Weights - Dataset2 ===\n",
      "\n",
      "  Classifier  TF-IDF  BoW  Word2Vec-CBOW  FastText  Skip-gram  GloVe\n",
      "     SVM-RBF    0.37 0.39           0.36      0.35       0.37   0.36\n",
      "RandomForest    0.28 0.26           0.27      0.28       0.27   0.28\n",
      "  ExtraTrees    0.21 0.20           0.21      0.21       0.20   0.21\n",
      "     XGBoost    0.14 0.15           0.16      0.16       0.16   0.15\n",
      "\n",
      "=== Uncertainty-Aware Ensemble Performance (10-Fold CV) - Dataset2 ===\n",
      "\n",
      "    Embedding  Accuracy  Precision  Recall    F1  LogLoss  KLMean\n",
      "       TF-IDF     0.976      0.974   0.970 0.972    0.221   0.055\n",
      "          BoW     0.969      0.967   0.962 0.965    0.236   0.061\n",
      "Word2Vec-CBOW     0.961      0.959   0.954 0.956    0.247   0.067\n",
      "     FastText     0.964      0.962   0.957 0.959    0.242   0.063\n",
      "    Skip-gram     0.958      0.956   0.950 0.953    0.253   0.070\n",
      "        GloVe     0.951      0.949   0.943 0.946    0.264   0.076\n",
      "\n",
      "✅ Non-linear classifier experiment for Dataset2 completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# DATASET 1 : Embedding + Non-Linear Classifier + Uncertainty Ensemble\n",
    "# =====================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "from scipy.stats import entropy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =================================================\n",
    "# Evaluation with LogLoss & KL Divergence\n",
    "# =================================================\n",
    "def evaluate_uncertainty(model, X, y, cv=10):\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    y_probs = cross_val_predict(model, X, y, cv=skf, method=\"predict_proba\")\n",
    "    y_pred = np.argmax(y_probs, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    prec = precision_score(y, y_pred, average=\"macro\")\n",
    "    rec = recall_score(y, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y, y_pred, average=\"macro\")\n",
    "    ll = log_loss(y, y_probs)\n",
    "    mean_kl = np.mean([entropy([1 if yi == c else 0 for c in np.unique(y)], y_probs[i])\n",
    "                       for i, yi in enumerate(y)])\n",
    "    return acc, prec, rec, f1, ll, mean_kl\n",
    "\n",
    "# =================================================\n",
    "# Non-linear Model definitions\n",
    "# =================================================\n",
    "models = {\n",
    "    \"SVM-RBF\": SVC(kernel='rbf', probability=True),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=200, max_depth=None),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=200, max_depth=None),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# =================================================\n",
    "# Embedding computation\n",
    "# =================================================\n",
    "embeddings = {\n",
    "    \"TF-IDF\": get_tfidf().fit_transform(X_train),\n",
    "    \"BoW\": get_bow().fit_transform(X_train),\n",
    "    \"Word2Vec-CBOW\": get_word2vec(X_train, sg=0),\n",
    "    \"Skip-gram\": get_word2vec(X_train, sg=1),\n",
    "    \"FastText\": get_fasttext(X_train),\n",
    "    \"GloVe\": get_glove(X_train, \"glove.6B.300d.txt\")\n",
    "}\n",
    "\n",
    "# =================================================\n",
    "# Run Experiments\n",
    "# =================================================\n",
    "results = []\n",
    "for emb_name, X_emb in embeddings.items():\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"=== {emb_name} + {model_name} ===\")\n",
    "        acc, prec, rec, f1, ll, kl = evaluate_uncertainty(model, X_emb, y_train, cv=10)\n",
    "        results.append([emb_name, model_name, acc, prec, rec, f1, ll, kl])\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\n",
    "    \"Embedding\", \"Classifier\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"LogLoss\", \"MeanKL\"\n",
    "])\n",
    "\n",
    "# =================================================\n",
    "# Pivot the Log-Loss & KL results (for display)\n",
    "# =================================================\n",
    "df_pivot = df_results.pivot(index='Embedding', columns='Classifier', values=['LogLoss', 'MeanKL'])\n",
    "df_pivot.columns = [f\"{col2}_{col1}\" for col1, col2 in df_pivot.columns]\n",
    "df_pivot = df_pivot.reset_index()\n",
    "print(\"\\n=== Log-Loss and Mean Kullback–Leibler (KL) Divergence - Dataset2 ===\\n\")\n",
    "print(df_pivot.round(3))\n",
    "\n",
    "# =================================================\n",
    "# Compute KL-Inverse Weighted Ensemble\n",
    "# =================================================\n",
    "ensemble_weights = []\n",
    "for clf in models.keys():\n",
    "    sub = df_results[df_results[\"Classifier\"] == clf]\n",
    "    kl_vals = sub[\"MeanKL\"].values\n",
    "    inv_kl = 1 / (kl_vals + 1e-8)\n",
    "    weights = inv_kl / inv_kl.sum()\n",
    "    ensemble_weights.append([clf] + list(np.round(weights, 2)))\n",
    "\n",
    "emb_names = list(embeddings.keys())\n",
    "ensemble_df = pd.DataFrame(ensemble_weights, columns=[\"Classifier\"] + emb_names)\n",
    "print(\"\\n=== KL-Inverse Weighted Ensemble Weights - Dataset2 ===\\n\")\n",
    "print(ensemble_df)\n",
    "\n",
    "# =================================================\n",
    "# Aggregate Ensemble Performance per Embedding\n",
    "# =================================================\n",
    "agg = (df_results.groupby(\"Embedding\")[[\"Accuracy\",\"Precision\",\"Recall\",\"F1\",\"LogLoss\",\"MeanKL\"]]\n",
    "       .mean().reset_index())\n",
    "agg.rename(columns={\"F1\": \"F1-Score\"}, inplace=True)\n",
    "\n",
    "print(\"\\n===Uncertainty-Aware Ensemble Performance (10-Fold CV) - Dataset2===\\n\")\n",
    "print(agg.round(4))\n",
    "\n",
    "print(\"\\n✅ Non-linear classifier experiment for Dataset2 completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc7954-a54a-4e55-92f7-e9acfe0c4bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
